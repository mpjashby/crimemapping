---
title: "Crime Mapping: Mapping area data"
description: "Learn how to make maps that show data for areas, as well as how to produce interactive maps"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)

# Load packages
library(ggmap)
library(sf)
library(tmap)
library(tidyverse)

# Load data --------------------------------------------------------------------

# NYC data
shootings <- read_csv("../../extdata/nyc_shootings.csv") %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
precincts <- read_sf("https://opendata.arcgis.com/datasets/c35786feb0ac4d1b964f41f874f151c1_0.geojson") %>% 
  janitor::clean_names()
shooting_counts <- st_join(
  st_transform(shootings, 2263), 
  st_transform(select(precincts, objectid), 2263)
) %>% st_set_geometry(NULL) %>% 
  count(objectid)

# Uttar Pradesh data
murders <- read_csv("../../extdata/uttar_pradesh_murders.csv")
districts <- read_sf("../../extdata/uttar_pradesh_districts.gpkg")
district_pop <- read_csv("../../extdata/uttar_pradesh_population.csv")
district_murders <- left_join(districts, murders, by = c("state" = "state", "district_name" = "district"))
district_murders_pop <- left_join(district_murders, district_pop, by = c("district_name" = "district")) %>% 
  mutate(murder_rate = murder / (population / 100000))
```


## Introduction

In previous tutorials we have produced maps based on the locations of individual
crimes, sometimes known as *point data* because we know the point at which each
crime occurred. But sometimes we will want to not map individual crimes but 
instead map counts of crimes for different areas. There are different reasons
why we might want to do this:

  * We might want to compare the number of crimes in different areas, such as
    police districts.
  * We might want to estimate the relative risk of a crime occurring in 
    different places by calculating crime rates using counts of crimes and 
    population data.
  * We might only have access to counts of crimes for different areas, rather
    than the location of each crime, perhaps in order to protect the privacy of
    crime victims.
    
In all these cases we need to make maps of crimes for different areas. These are
sometimes called *thematic maps* or *choropleth maps*. In this tutorial we will 
learn how to produce this thematic map of murder rates in the Indian state of
Uttar Pradesh, and to think about some of the new issues that producing 
choropleth maps creates.

```{r intro-exercise1, message=FALSE, out.width="100%"}
tmap_mode("view")

tm_shape(district_murders_pop) +
  tm_basemap("Stamen.TonerBackground") +
  tm_polygons("murder_rate", alpha = 0.75, title = "murder rate") +
  tm_tiles("Stamen.TonerLabels")
```



## Counting crimes

Sometimes we will want to aggregate the locations of crimes to counts of crime
for different areas. To do this, we need a dataset representing the crime
locations and another representing the boundaries of the areas that we want to
calculate counts for. In this example, we will return to the New York City
shooting data that we have studied in a previous tutorial to count the number of
recorded fatal and non-fatal shootings in each police precinct in the year 2019.


### Load the data

First, we load the shootings data from the URL `https://github.com/mpjashby/crimemapping/raw/main/inst/extdata/nyc_shootings.csv` -- 
write the R code needed to load this file into an R object called `shootings`
and convert it into an SF object that uses the WGS84 co-ordinate reference 
system (EPSG code 4326). Remember to load any packages that you need first.

```{r count-exercise1, exercise=TRUE, exercise.lines=6}

```

```{r count-exercise1-solution}
library(sf)
library(tidyverse)

shootings <- read_csv("https://github.com/mpjashby/crimemapping/raw/main/inst/extdata/nyc_shootings.csv") %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

shootings
```

We also need to load the precinct boundaries as an SF object from the URL 
`https://opendata.arcgis.com/datasets/c35786feb0ac4d1b964f41f874f151c1_0.geojson`, 
store it in an object called `precincts` and change the column names to
comply with the code style that we introduced in a previous tutorial. Write the 
R code needed to do this. Use any notes you have from previous tutorials to help 
you if you need them, or go back to the tutorial called 'Giving a map context' 
to refresh your memory.

```{r count-exercise2, exercise=TRUE}

```

```{r count-exercise2-solution}
precincts <- read_sf("https://opendata.arcgis.com/datasets/c35786feb0ac4d1b964f41f874f151c1_0.geojson") %>% 
  janitor::clean_names()
```


### Count crimes in each precinct

Now that we have loaded our data, we can count the number of shootings in each
precinct and produce an R object that contains the boundaries of the precincts
together with the number of shootings in each one. To do this, we will:

  1. Identify which precinct each crime is in using the `st_join()` function 
     from the `sf` package.
  2. Count the number of crimes in each precinct using the `count()` function 
     from the `dplyr` package.
  3. Join the counts of crimes to the precinct boundaries using the 
     `left_join()` function from the `dplyr` package.

The `st_join()` function in the `sf` package takes two spatial layers, one 
containing points and one containing polygons representing areas, and joins all
the values in the polygon layer to every point that falls within each area.

We do not need to add all the columns in the `precincts` object to every row in
the `shootings` object -- we just need to know which precinct each shooting
occurred in. If we look at the contents of the `precincts` object, we can see
that it contains a column called `objectid` that is unique to every row in the
data. We will use this column to match our crime counts to each precinct at step
3 of this process, so it is important that we choose a column for which every 
row has a unique value. To match only this column, we will use the `select()`
function from `dplyr` to remove all the other columns before we use `st_join()`.



```{r count-exercise3, exercise=TRUE}
shootings_in_precincts <- st_join(shootings, select(precincts, objectid))

shootings_in_precincts
```

As well as producing a joined dataset as expected, this code has produced a
message noting that `st_join()` assumes that co-ordinates are planar even though
they are specified as latitudes and longitudes. This message is there to remind
us that `st_join()` uses certain assumptions about the shape of the earth that
might make identifying which precinct each shooting occurred in incorrect in
some circumstances. This is largely a problem for maps covering very-large areas
or places near the North or South poles. To be on the safe side, we could first
transform both the spatial layers into a local co-ordinate system that was
appropriate for New York City. For example, we could use the New York Long 
Island State Plane co-ordinate system, which has the EPSG code 2263. Write code
that uses the `st_transform()` function to transform both layers, join them and
display the result.

```{r count-exercise4, exercise=TRUE}

```

```{r count-exercise4-solution}
shootings <- st_transform(shootings, 2263)
precincts <- st_transform(precincts, 2263)

shootings_in_precincts <- st_join(shootings, select(precincts, objectid))

shootings_in_precincts
```

We can see that this time, no messages are produced.

If you look at the `shootings_in_precincts` object, you will see that there is
a column showing the value of `objectid`, i.e. which row in the `precincts`
dataset the location of the shooting corresponds to. We will use this column to
count the number of shootings in each precinct using the `count()` function from
the `dplyr` package. Running `count()` on SF objects can sometimes be slow, so
before we do this we will remove the geometry column from the `shootings_in_precincts`
object using the `st_set_geometry()` function to set the geometry to the special
value `NULL`.

```{r count-exercise5, exercise=TRUE, exercise.setup="count-exercise3"}
shooting_counts <- shootings_in_precincts %>% 
  st_set_geometry(NULL) %>% 
  # call the column that holds the shooting counts `shootings`
  count(objectid, name = "shootings")

shooting_counts
```

The object `shooting_counts` has two rows: one showing the `objectid` 
corresponding to each precinct and one showing the number of shootings occurring
there. Note that this object contains only those precincts in which at least one
shooting occurred, since we are counting shootings, not precincts.

To create a dataset containing the boundary of each precinct and the
relevant shooting count, we will join `shooting_counts` to the existing
`precincts` object using the `left_join()` function.

`left_join()` is one of a family of joining functions in the `dplyr` package 
that join to objects together based on the values of particular columns. We are
using the `left_join()` function because we want the result to contain *all* the
rows from the *left-hand* object that we specify, i.e. the object `precincts` in 
the code `left_join(precincts, shooting_counts)`. If there are any rows in the 
right-hand object `shooting_counts` that do not match any rows in the left-hand 
object `precincts`, those rows will be missing from the result. If we instead 
wanted to keep all the rows from `shooting_counts` and only the matching rows 
from `precincts`, we would use the `right_join()` function (or reverse the order
of the two arguments). If we wanted to keep all rows from both datasets whether 
they matched each other or not, we could use the `full_join()` function.

<a href="https://tidyr.tidyverse.org/" title="tidyr website"><img src="images/tidyr.png" style="width: 33%; max-width: 150px; float: right; margin: 0 0 2em 2em;"></a>

By default, `left_join()` will match the rows in the `precincts` and 
`shooting_counts` objects using all the column names that are present in both
datasets. This can sometimes have unexpected results, so it is safer to specify
which columns we want to match to be based on using the `by` argument to 
`left_join()`, in this case to specify `by = "objectid"`.

We are choosing to keep all the rows from `precincts` and only those matching
rows from `shooting_counts` because we want our resulting dataset to include all
precincts, even those (if any) in which there were no shootings. Any precincts
with no shootings will have missing values in the dataset produced by 
`left_join()`, so we will replace these with zeros using the `replace_na()`
function from the `tidyr` package (another part of the tidyverse).

```{r count-exercise7, exercise=TRUE, exercise.setup="count-exercise5"}
precinct_counts <- left_join(precincts, shooting_counts, by = "objectid") %>% 
  # replace missing (NA) values in the `shootings` column with the value 0
  replace_na(list(shootings = 0))

precinct_counts
```


We now have an SF object containing the boundary of each police precinct in New
York City and the number of shootings occurring there. We could use this object
to make a choropleth map of shootings, but before we do this we need to 
understand some of the limitations of this sort of map.



## Mapping areas

Maps showing counts of crimes, or a wide range of other data, for different 
areas are extremely common in all types of spatial analysis. Choropleth maps can
be produced for all sorts of areas -- watch this video to find out more about 
the types of areas for which the statistical data used to produce maps like 
these are often produced.

![](https://youtu.be/d2zLCSl4PF8)


### Two problems with area maps

Because so much of the data that we might use in crime mapping is only available
for specific areas, we often have no choice as to what types of area we use to
produce our maps. But whether we choose the areas or not, we have to be mindful
that data for areas can only tell us about the area as a whole, not individual
people or places within that area. Assuming information about areas applies to 
individual people or places within them is known as the *ecological fallacy*, a
type of logical error that can be dangerous when we try to draw conclusions from
choropleth maps. Watch this video to find out more about the ecological fallacy.

![](https://youtu.be/xpJEBVO5AGM)

There is a second potential problem when we use choropleth maps to counts crimes
or other data that relate to specific points within an area. This is known as
the modifiable areal unit problem, or MAUP. The MAUP describes how the apparent
distribution of crime counts on choropleth maps is determined not just by the 
actual locations of crimes but also the choice of areas that we use to do the
counting. Watch this video to learn more about the MAUP.

![](https://youtu.be/7NTw1v7DuAA)

Neither the ecological fallacy nor the MAUP are problems that we can solve --
they are inherent to choropleth maps. For this reason, whenever we plan to show
some spatial data using a choropleth map, it is worth stopping and thinking 
about whether another type of map (such as a density map) might show the data
more effectively and without the problems that choropleth maps have. 
Nevertheless, in some circumstances a choropleth map will be the best choice 
either because we only have data for pre-defined areas or because we are 
specifically interested in comparing areas (e.g. in working out if there is more
crime in one neighbourhood or police district than another).



## Making a choropleth map

In this section we will make a choropleth map of murders in different districts
within the state of Uttar Pradesh in northern India in 2014. Uttar Pradesh is 
the largest state in India, with more than 200 million residents.


### Loading and joining datasets

First, we load the counts of murders in each district and the district 
boundaries as separate objects.

```{r choro-exercise1, exercise=TRUE}
library(sf)
library(tidyverse)

murders <- read_csv("https://github.com/mpjashby/crimemapping/raw/main/inst/extdata/uttar_pradesh_murders.csv")
districts <- read_sf("https://github.com/mpjashby/crimemapping/raw/main/inst/extdata/uttar_pradesh_districts.gpkg")

message("First few rows of the `murders` object …")
head(murders, n = 3)

message("First few rows of the `districts` object …")
head(districts, n = 3)
```

Note that we have used the `message()` function here to make it easier to 
decipher the output of this code, since we are printing both the `murders` and
`districts` objects. We have also used the `head()` function to print just the
first three rows of each dataset.

We will need to join these two objects using `left_join()` to create a
choropleth map. From the output above we can see that the `murders` object 
contains the count of murders in each district and the name of the district in 
the `district` column. The `districts` object also contains the name of each 
district, but in a column called `district_name`. To join the two datasets based 
on this column, we will have to specify that we want to use those differently 
named columns using the argument `by = c("district_name" = "district")`. As in
the New York City shootings example earlier, we will use `left_join()` and 
specify the `districts` object as the first argument in order to keep all the
districts in the resulting object, whether there were any murders in the 
district or not.

```{r choro-exercise2, exercise=TRUE}
district_murders <- left_join(districts, murders, by = c("district_name" = "district"))

district_murders
```

Looking at the `district_murders` object, we can see that each row now contains
the name of a district and the number of murders that occurred there. There are
no districts in which there were zero murders, so we do not need to replace any
`NA` values with zeros using `replace_na()` as in the previous example.

You may have noticed that the `district_murders` object has two columns named
`state.x` and `state.y`. This is because both the datasets we have joined 
together contained a column called `state`, but those columns were not used to
join the two datasets together. We can avoid these duplicate columns by:

  * using `select()` *before* using `left_join()` to remove the `state` column
    from one dataset or the other, e.g. `select(murders, -state)`,
  * using `select()` *after* using `left_join()` to remove one of the duplicate
    columns and to rename the other, e.g. 
    `select(district_murders, state = state.x, -state.y)`, or
  * adding `state` as a joining variable in `left_join()`, e.g.
    `left_join(districts, murders, by = c("state" = "state", "district_name" = "district"))`.

All three of these options produce the same result.


### Making a choropleth map

The `district_murders` object we have produced is an SF object, which means we
can use `geom_sf()` in combination with `ggmap()` to plot the districts on top 
of a base map.

```{r choro-exercise3, exercise=TRUE, exercise.lines=17, message=FALSE, out.width="90%", fig.asp=1}
library(ggmap)

district_murders %>% 
  # find the bounding box of the data
  st_bbox() %>% 
  # change the names of the values to those expected by `get_stamenmap()`
  set_names(c("left", "bottom", "right", "top")) %>% 
  # download the base map data
  get_stamenmap(zoom = 8, maptype = "toner") %>% 
  # plot the base map, remembering that `ggmap()` produces a `ggplot()` stack
  ggmap() +
  # add the districts layer, remembering `inherit.aes = FALSE` to prevent an
  # `object 'lon' not found` error
  geom_sf(data = district_murders, inherit.aes = FALSE) +
  # remove unnecessary contextual element from the map
  theme_void() +
  theme(panel.border = element_rect(colour = "black", fill = NA))
```

There are at least three problems with this map: 

  1. we cannot tell how many murders there were in each district,
  2. we cannot see the base map under the polygons representing the districts,
     and,
  3. the districts layer is slightly mis-aligned on top of the base map.

We can solve the first two problems by changing the aesthetics for the 
`geom_sf()` layer in our code. Specifically, we can make the district polygons
semi-transparent by setting the `alpha` argument to `geom_sf()` to a value of 
less than one, while also using `aes(fill = murder)` to specify that the fill
colour of the polygons should depend on the number of murders in each district.
At the same time we will use `scale_fill_brewer()` to use a colour scheme that
is accessible to the widest range of users.

```{r choro-exercise4, exercise=TRUE, exercise.lines=25, message=FALSE, out.width="90%", fig.asp=1}
district_murders %>% 
  # find the bounding box of the data
  st_bbox() %>% 
  # change the names of the values to those expected by `get_stamenmap()`
  set_names(c("left", "bottom", "right", "top")) %>% 
  # download the base map data
  get_stamenmap(zoom = 8, maptype = "toner") %>% 
  # plot the base map, remembering that `ggmap()` produces a `ggplot()` stack
  ggmap() +
  # add the districts layer, remembering `inherit.aes = FALSE` to prevent an
  # `object 'lon' not found` error
  geom_sf(
    aes(fill = murder), 
    data = district_murders, 
    inherit.aes = FALSE, 
    alpha = 0.75
  ) +
  # use a better colour scheme
  scale_fill_distiller(palette = "Oranges", direction = 1) +
  # add some labels
  labs(fill = "number of\nmurders") +
  # remove unnecessary contextual element from the map
  theme_void() +
  theme(panel.border = element_rect(colour = "black", fill = NA))
```

This map is better, but we haven't yet solved the mis-alignment problem. This
only happens when we try to plot very large areas using `ggmap()` -- for crime
maps of local areas such as individual cities, this problem does not occur. But
for mapping larger areas such as large states or entire countries, we must make
sure that our map layers align with one another. To do this, we will use a 
different package called `tmap`.



## Mapping with `tmap`

The functions in the `tmap` package work in a similar way to `ggplot()` and
`ggmap()`, but with a few differences. Fortunately, `tmap` can do a few things
that the mapping packages we have looked at already cannot.

Before using the `tmap` package, remember to install it on your computer using
the `install.packages()` function. We can create a very basic map using 
functions from the `tmap` package by using `tm_shape()` to define the SF object
that will be used to draw shapes on our map and then `tm_polygons()` to 
specify the column in that object that the colour of the shapes should be based
on.

```r
library(tmap)

tm_shape(district_murders) +
  tm_polygons("murder", alpha = 0.75, title = "number of murders")
```

```{r tmap-exercise1, message=FALSE, out.width="80%"}
library(tmap)

tmap_mode("plot")

tm_shape(district_murders) +
  tm_polygons("murder", alpha = 0.75, title = "number of murders")
```

One of the big advantages of using `tmap` is that it can produce interactive
maps. This is useful for maps of large areas such as Uttar Pradesh because it
allows us to zoom in on smaller areas of interest. If you found the labels on
the map in the previous section hard to read, this is the way to make them
easier to see.

To specify that we want to produce an interactive map, we call the function
`tmap_mode("view")` before starting (and separately from) our stack of `tmap`
functions. We can then use `tm_basemap()` to add a base map layer underneath the
districts. In this example we continue to use the 'toner' style of base map from 
Stamen that we have used before, but `tmap` has a very large number of options 
for base maps that you can [view in this online gallery](https://leaflet-extras.github.io/leaflet-providers/preview/).
We can also use the `tmap_tiles()` function to add a another base map layer
*above* our data (if it is added to the stack after the `tm_polygons()` layer),
which is useful for showing labels such as the names of cities above the data.

```r
tmap_mode("view")

tm_shape(district_murders) +
  tm_basemap("Stamen.TonerBackground") +
  tm_polygons("murder", alpha = 0.75, title = "number of murders") +
  tm_tiles("Stamen.TonerLabels")
```

```{r tmap-exercise2, message=FALSE, out.width="100%"}
tmap_mode("view")

tm_shape(district_murders) +
  tm_basemap("Stamen.TonerBackground") +
  tm_polygons("murder", alpha = 0.75, title = "number of murders") +
  tm_tiles("Stamen.TonerLabels")
```

This map allows you to move around as well as zooming in and out. This is very
useful for maps where people may need to see both overall patterns for a large
area and details for a smaller area within it. Obviously, interactive maps only
work on screens, not for printed maps.

Once we have specified that we want `tmap` to produce interactive maps, all maps
produced using `tmap` functions will be interactive. To change back to static
maps that are suitable for printing, we can use `tmap_mode("plot")`.



## Calculating crime rates

One use of choropleth maps is to combine crime counts with other data. One 
reason to do this is to calculate a crime rate, i.e. an expression of the 
frequency of crime that in some way controls for the population that is exposed
to crime.

Before we learn about different types of crime rate, lets create a *targeting
table* for murder in districts in Uttar Pradesh.

```{r rates-exercise1, exercise=TRUE}
district_murders %>% 
  # remove the `geometry` column to make the table clearer
  st_set_geometry(NULL) %>% 
  # also remove the `state` column, since it is the same for every row
  select(-state) %>% 
  arrange(desc(murder))
```

```{r}
district_murders_ranked <- district_murders %>% 
  st_set_geometry(NULL) %>% 
  arrange(desc(murder)) %>% 
  pluck("district_name")
```

From this, we can see that the highest number of murders in 2014 occurred in the
`r nth(district_murders_ranked, 1)`, `r nth(district_murders_ranked, 2)` and 
`r nth(district_murders_ranked, 3)` districts.


### Types of crime rate

There are three types of crime rate, each of which measures risk in a different
way. The most common, and the type of rate that people almost always mean if
they talk about a 'crime rate' without specifying another type, is the 
*incidence rate*. This is the number of crimes in an area divided by the number
of people, households or places against which a crime could occur. For example,
the incidence rate of burglary in an area might be calculated as the number of
crimes per 1,000 households, while the incident rate of homicide might be 
expressed as the number of homicides per 100,000 people.

$$
\textrm{incidence} = \frac{\textrm{crimes}}{\textrm{population}}
$$

Incidence rates are useful for comparing the risk from crime in areas of 
different sizes. For example, knowing that `r nth(district_murders_ranked, 1)`
had more murders in 2014 than any other district in Uttar Pradesh might be 
useful in deciding where to send more homicide detectives, but it doesn't tell
us much about the risk of being murdered in `r nth(district_murders_ranked, 1)`
because we don't know if this district has more murders simply because it has
more people.

Incidence rates are useful for comparing areas, but they tell us a little
about how likely an individual is to be a victim of crime because crime is 
heavily concentrated against a few victims (think back to the law of crime
concentration we discussed in a previous tutorial). To understand the average
risk of a person being a victim of crime once or more, we calculate the 
*prevalence rate*. This is the number of people, households, etc who were 
victims at least once, divided by the total number of people, households, etc. 
Prevalence rates are usually expressed as a percentage, so we might say that 1% 
of the population has been a victim of a particular crime at least once during a 
year.

$$
\textrm{prevalence} = \frac{\textrm{victims}}{\textrm{population}}
$$

The final type of rate is the *concentration rate*. This tells us how 
concentrated crime is, and is usually expressed as a single number, e.g. if
the concentration rate for a crime is 2.5 then we can say that everyone who was 
a victim of that crime at least once was on average victimised 2.5 times. This 
is particularly useful for understanding how important repeat victimisation is
to driving up the frequency of crime in an area. This, in turn, might lead us to
focus crime-prevention efforts on work to protect recent victims of crime from
being victimised again.

$$
\textrm{concentration} = \frac{\textrm{crimes}}{\textrm{victims}}
$$

All three types of crime rate are average values for an area as a whole, so we 
are always at risk of committing the ecological fallacy. Remember that while
rates are useful for describing areas, that does not imply everyone in an area
faces the same risk from crime.


### Choosing a population measure

To calculate a crime rate we need to be able to measure the population that is
at risk from that crime. It is very common for analysts to calculate rates based
on the number of people who live in an area, not least because that information
is often easily available. However, there are many situations in which the 
residential population of an area is a poor measure of the population at risk 
for crime there. For example:

  * Robberies in a shopping area where most of the victims do not live in the 
    area but instead travel from elsewhere to go shopping. This can lead to 
    vastly inflated crime rates for commercial and entertainment districts that
    have very small residential populations but very large numbers of people
    coming into the area to work, shop or visit.
  * Assaults on public transport where victims happen to be passing through a
    given area at the time they are victimised but are doing so as part of a
    longer journey through several areas, with the crime potentially occurring
    in any one of them.
  * Residential burglaries where the targets of crime are homes, not people, so
    the crime rate may be influenced by the number of people in each household.

In all these cases, the residential population is a poor measure of the 
population at risk from crime. Unfortunately, other measures of population (such
as counts of people on public transport or walking along a shopping street) are
often expensive or difficult to obtain. This is known as the *denominator 
dilemma*: should we use residential population to calculate crime rates just 
because it is the only data that is available?


### Calculating murder rates in Uttar Pradesh

In the case of murder in Uttar Pradesh, residential population is likely to be
an acceptable (although not perfect) denominator to use in calculating an
incidence rate. This is because Indian districts are relatively large areas and
so we can expect that most murder victims will be killed in the district where
they live (because people everywhere tend to spend most of their time close to
where they live). However, there will definitely be exceptions to this, with
people being murdered in a different district to where they live. Working out
the extent of this problem would be an important question for a detailed 
investigation into murder in Uttar Pradesh, but we will not consider it any
further here.

To calculate an incidence rate, we need to join population data to the existing
`district_murders` object containing murder counts and district outlines. Write
the code needed to download population data from 
`https://github.com/mpjashby/crimemapping/raw/main/inst/extdata/uttar_pradesh_population.csv`
and join it to the `district_murders` object, saving the result as 
`district_murders_pop`.

```{r rates-exercise2, exercise=TRUE}

```

```{r rates-exercise2-solution}
district_pop <- read_csv("https://github.com/mpjashby/crimemapping/raw/main/inst/extdata/uttar_pradesh_population.csv")
  
district_murders_pop <- left_join(
  district_murders, 
  district_pop, 
  by = c("district_name" = "district")
)

district_murders_pop
```

Now that we have a single dataset containing all the variables we need, we can
calculate the incidence of murders per 100,000 people.

```{r rates-exercise3, exercise=TRUE}
district_murders_pop <- district_murders_pop %>% 
  mutate(murder_rate = murder / (population / 100000)) %>% 
  # at the same time we will remove unnecessary variables to clean up the data
  select(district_name, murder, population, murder_rate)

district_murders_pop
```

To create a choropleth map of the murder rate using `tmap`, we simply use the
code from our previous interactive map but specify that the `murder_rate`
column be used to determine the colour of each district polygon rather than the
`murder` column. We will also change the base map to a different style.

```r
tmap_mode("view")

tm_shape(district_murders_pop) +
  tm_basemap("Esri.WorldImagery") +
  tm_polygons("murder_rate", alpha = 0.75, title = "number of murders") +
  tm_tiles("Stamen.TonerLabels")
```

```{r rates-exercise4, message=FALSE, out.width="100%"}
tmap_mode("view")

tm_shape(district_murders_pop) +
  tm_basemap("Esri.WorldImagery") +
  tm_polygons("murder_rate", alpha = 0.75, title = "murder rate") +
  tm_tiles("Stamen.TonerLabels")
```

It may be useful to compare the count of murder and murder rate across each 
district. We can do this by making a scatter plot with `ggplot()`, using
`geom_point()` to draw the points and `geom_smooth(method = "lm")` to draw a
straight trend line for the relationship between the two variables (you can
ignore the argument `formula = "y ~ x"` -- this is the default value for this
argument, but `ggplot()` produces a message if it is not specified explicitly.

```{r rates-exercise5, exercise=TRUE}
ggplot(district_murders_pop, aes(murder, murder_rate)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  scale_x_continuous(limits = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA)) +
  labs(
    title = "Murders in districts in Uttar Pradesh, 2014", 
    x = "number of murders", 
    y = "rate of murders per 100,000 people"
  ) +
  theme_minimal()
```

From this chart we can see that most districts have low counts of murders and
low murder rates. This is as we would expect it to be, because we know that 
crime is concentrated in a few places. We can also see that there a few 
districts with high murder counts, and some districts which do not have
especially high murder counts but do have very high murder rates. We can see on
the map that the districts with higher murder rates tend to be concentrated 
along the border with the Indian capital New Delhi (zoom into the map to see
this in more detail).



## In summary

In this tutorial we have learned how to:

  * calculate the number of points in different areas,
  * join different sources of data together, and
  * map crime counts and crime rates on both static and interactive maps.

We have also learned about two disadvantages of choropleth maps: the ecological
fallacy and the modifiable areal unit problem.

To find out more about the topics covered in this tutorial:

  * Read the article [Crime seen through a cone of resolution](https://journals.sagepub.com/doi/pdf/10.1177/000276427602000207) by
    Paul Brantingham, Delmar Dyreson and Patricia Brantingham for more detail
    about the ecological fallacy in studying crime.
  * Read the article [Smallest is Better? The Spatial Distribution of Arson and the Modifiable Areal Unit Problem](https://doi.org/10.1007/s10940-016-9297-6) for an example of the modifiable areal unit problem in studying crime.


